{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2023-03-01T06:04:05.795051Z","iopub.status.busy":"2023-03-01T06:04:05.793968Z","iopub.status.idle":"2023-03-01T06:04:05.801960Z","shell.execute_reply":"2023-03-01T06:04:05.801096Z","shell.execute_reply.started":"2023-03-01T06:04:05.795006Z"},"trusted":true},"outputs":[],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Residual Networks\n","In this notebook we are going to build a very deep convolutional network, using Residual Networks (ResNets). In theory, very deep networks can represent very complex functions; but in practice, they are hard to train. Residual Networks, introduced by [He et al.](https://arxiv.org/pdf/1512.03385.pdf), allow you to train much deeper networks than were previously feasible."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Import Packages"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:05.805090Z","iopub.status.busy":"2023-03-01T06:04:05.804355Z","iopub.status.idle":"2023-03-01T06:04:08.216978Z","shell.execute_reply":"2023-03-01T06:04:08.215757Z","shell.execute_reply.started":"2023-03-01T06:04:05.805053Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","from torchvision import datasets\n","from torchvision import transforms\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Problem of Very Deep Neural Networks\n","In recent years, neural networks have become much deeper, with state-of-the-art networks evolving from having just a few layers (e.g., AlexNet) to over a hundred layers.\n","\n","* The main benefit of a very deep network is that it can represent very complex functions. It can also learn features at many different levels of abstraction, from edges (at the shallower layers, closer to the input) to very complex features (at the deeper layers, closer to the output). \n","\n","* However, using a deeper network doesn't always help. A huge barrier to training them is vanishing gradients: very deep networks often have a gradient signal that goes to zero quickly, thus making gradient descent prohibitively slow.\n","\n","* More specifically, during gradient descent, as you backpropagate from the final layer back to the first layer, you are multiplying by the weight matrix on each step, and thus the gradient can decrease exponentially quickly to zero (or, in rare cases, grow exponentially quickly and \"explode,\" from gaining very large values). \n","\n","* During training, you might therefore see the magnitude (or norm) of the gradient for the shallower layers decrease to zero very rapidly as training proceeds."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Building a Residual Network\n","In ResNets, a \"shortcut\" or a \"skip connection\" allows the model to skip layers:  \n","\n","<center><img src=\"Images/skip_connection.png\" style=\"width:650px;height:200px;\"></center>\n","<caption><center> <u> <font color='purple'> <b>Figure 1</b> </u><font color='purple'>  : A ResNet block showing a skip-connection <br> </center></caption>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The image on the left shows the \"main path\" through the network. The image on the right adds a shortcut to the main path. By stacking these ResNet blocks on top of each other, you can form a very deep network."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### The Identity Block"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The identity block is the standard block used in ResNets, and corresponds to the case where the input activation (say $a^{[l]}$) has the same dimension as the output activation (say $a^{[l+2]}$).\n","\n","In the code cell below, we are going to implement a slightly more powerful version of this identity block, in which the skip connection \"skips over\" 3 hidden layers rather than 2 layers. It looks like this: \n","\n","<center><img src=\"Images/idblock3_kiank.png\" style=\"width:650px;height:150px;\"></center>"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:08.219145Z","iopub.status.busy":"2023-03-01T06:04:08.218619Z","iopub.status.idle":"2023-03-01T06:04:08.231726Z","shell.execute_reply":"2023-03-01T06:04:08.230463Z","shell.execute_reply.started":"2023-03-01T06:04:08.219110Z"},"trusted":true},"outputs":[],"source":["class IdentityBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride = 1):\n","        super().__init__()\n","        F1, F2, F3 = out_channels\n","        self.conv1 = nn.Sequential(\n","                        nn.Conv2d(in_channels, F1, kernel_size = 1, stride = stride, padding = 'valid'),\n","                        nn.BatchNorm2d(F1),\n","                        nn.ReLU())\n","        self.conv2 = nn.Sequential(\n","                        nn.Conv2d(F1, F2, kernel_size = kernel_size, stride = 1, padding = 'same'),\n","                        nn.BatchNorm2d(F2),\n","                        nn.ReLU())\n","        self.conv3 = nn.Sequential(\n","                        nn.Conv2d(F2, F3, kernel_size=1, stride=1, padding='valid'),\n","                        nn.BatchNorm2d(F3))\n","    \n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        out = self.conv3(out)\n","        out += residual\n","        out = self.relu(out)\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Convolution Block\n","The ResNet \"convolutional block\" is the second block type. You can use this type of block when the input and output dimensions don't match up. The difference with the identity block is that there is a CONV2D layer in the shortcut path:\n","<center><img src=\"images/convblock_kiank.png\" style=\"width:650px;height:150px;\"></center>"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:08.236176Z","iopub.status.busy":"2023-03-01T06:04:08.235852Z","iopub.status.idle":"2023-03-01T06:04:08.250266Z","shell.execute_reply":"2023-03-01T06:04:08.249207Z","shell.execute_reply.started":"2023-03-01T06:04:08.236142Z"},"trusted":true},"outputs":[],"source":["class ConvolutionBlock(nn.Module):\n","    def __init__(self, in_channels, out_channels, kernel_size=3, stride = 1):\n","        super().__init__()\n","        F1, F2, F3 = out_channels\n","        self.conv1 = nn.Sequential(\n","                        nn.Conv2d(in_channels, F1, kernel_size = 1, stride = stride, padding = 'valid'),\n","                        nn.BatchNorm2d(F1),\n","                        nn.ReLU())\n","        self.conv2 = nn.Sequential(\n","                        nn.Conv2d(F1, F2, kernel_size = kernel_size, stride = 1, padding = 'same'),\n","                        nn.BatchNorm2d(F2),\n","                        nn.ReLU())\n","        self.conv3 = nn.Sequential(\n","                        nn.Conv2d(F2, F3, kernel_size=1, stride=1, padding='valid'),\n","                        nn.BatchNorm2d(F3))\n","        \n","        self.shortcut_conv = nn.Sequential(\n","                        nn.Conv2d(in_channels, F3, kernel_size=1, stride=stride, padding='valid'),\n","                        nn.BatchNorm2d(F3))\n","    \n","        self.relu = nn.ReLU()\n","        \n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.conv2(out)\n","        out = self.conv3(out)\n","        residual = self.shortcut_conv(residual)\n","        out += residual\n","        out = self.relu(out)\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Residual Network Stages\n","We now have the necessary blocks to build a very deep ResNet. The following figure describes in detail the architecture of this neural network. \"ID BLOCK\" in the diagram stands for \"Identity block,\" and \"ID BLOCK x3\" means you should stack 3 identity blocks together.\n","<center><img src=\"images/resnet_kiank.png\" style=\"width:850px;height:150px;\"></center>"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:08.255510Z","iopub.status.busy":"2023-03-01T06:04:08.255190Z","iopub.status.idle":"2023-03-01T06:04:08.268985Z","shell.execute_reply":"2023-03-01T06:04:08.267416Z","shell.execute_reply.started":"2023-03-01T06:04:08.255486Z"},"trusted":true},"outputs":[],"source":["class ResidualStage(nn.Module):\n","\n","    def get_Identity_blocks(self):\n","        identity_blocks = []\n","        in_channel = self.out_channels[-1]\n","        for _ in range(self.n_identity):\n","            block = IdentityBlock(in_channel, self.out_channels, self.filter_size).to(device)\n","            identity_blocks.append(block)\n","        return identity_blocks \n","\n","    def __init__(self, n_identity, in_channels, out_channels, filter_size=3, stride=2):\n","        super().__init__()\n","        self.n_identity = n_identity\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.filter_size = filter_size\n","        self.stride = stride\n","        self.conv = ConvolutionBlock(in_channels, out_channels, filter_size, stride)\n","        self.identity_blocks = self.get_Identity_blocks()\n","\n","    def forward(self, x):\n","        out = self.conv.forward(x)\n","        for id_block in self.identity_blocks:\n","            out = id_block(out)\n","        return out"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Building ResNet Model (50 layers)"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:08.271441Z","iopub.status.busy":"2023-03-01T06:04:08.270606Z","iopub.status.idle":"2023-03-01T06:04:08.287119Z","shell.execute_reply":"2023-03-01T06:04:08.285932Z","shell.execute_reply.started":"2023-03-01T06:04:08.271406Z"},"trusted":true},"outputs":[],"source":["class ResNet50(nn.Module):\n","\n","    def __init__(self, n_classes):\n","        super().__init__()\n","        self.n_classes = n_classes\n","        self.stage_0 = nn.Sequential(\n","                       nn.Conv2d(3, 64, 7, 2, padding=3),\n","                       nn.BatchNorm2d(64),\n","                       nn.ReLU(),\n","                       nn.MaxPool2d(3, 2, 1)\n","        )\n","        self.stage_1 = ResidualStage(2, 64, [64, 64, 256], filter_size=3, stride=1).to(device)\n","        self.stage_2 = ResidualStage(3, 256, [128, 128, 512], filter_size=3, stride=2).to(device)\n","        self.stage_3 = ResidualStage(5, 512, [256, 256, 1024], filter_size=3, stride=2).to(device)\n","        self.stage_4 = ResidualStage(2, 1024, [512, 512, 2048], filter_size=3, stride=2).to(device)\n","        self.avgpool = nn.AvgPool2d(7, stride=1)\n","        self.flat = nn.Flatten()\n","        self.fc = nn.Linear(2048, self.n_classes)\n","        \n","    def forward(self, x):\n","        out = self.stage_0(x)\n","        out = self.stage_1(out)\n","        out = self.stage_2(out)\n","        out = self.stage_3(out)\n","        out = self.stage_4(out)\n","        out = self.avgpool(out)\n","        out = self.flat(out)\n","        out = self.fc(out)\n","        return out\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Load and Transform `CIFAR-10` dataset"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:08.291722Z","iopub.status.busy":"2023-03-01T06:04:08.291455Z","iopub.status.idle":"2023-03-01T06:04:15.623956Z","shell.execute_reply":"2023-03-01T06:04:15.622928Z","shell.execute_reply.started":"2023-03-01T06:04:08.291698Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f753348cef944b8082f9bf7ae7243ab7","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}],"source":["transform = transforms.Compose(\n","    [transforms.Resize((224,224)),\n","     transforms.ToTensor(),\n","     transforms.Normalize(\n","         mean=[0.4914, 0.4822, 0.4465],\n","         std=[0.2023, 0.1994, 0.2010])\n","    ])\n","\n","batch_size = 16\n","\n","trainset = datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Set Cost Function and Optimizer"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:15.625797Z","iopub.status.busy":"2023-03-01T06:04:15.625327Z","iopub.status.idle":"2023-03-01T06:04:18.112952Z","shell.execute_reply":"2023-03-01T06:04:18.111826Z","shell.execute_reply.started":"2023-03-01T06:04:15.625761Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of batches in training data : 3125\n"]}],"source":["num_classes = 10\n","num_epochs = 10\n","learning_rate = 0.01\n","\n","model = ResNet50(num_classes).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=0.001)  \n","\n","# Train the model\n","total_step = len(trainloader)\n","print(f\"Number of batches in training data : {total_step}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Train the Network"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2023-03-01T06:04:18.116029Z","iopub.status.busy":"2023-03-01T06:04:18.115644Z","iopub.status.idle":"2023-03-01T07:25:43.678263Z","shell.execute_reply":"2023-03-01T07:25:43.677119Z","shell.execute_reply.started":"2023-03-01T06:04:18.115992Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,  3125] training loss: 3.170051 validation loss: 3.385314\n","[2,  3125] training loss: 2.792747 validation loss: 2.458987\n","[3,  3125] training loss: 2.700126 validation loss: 2.192587\n","[4,  3125] training loss: 2.146834 validation loss: 1.952478\n","[5,  3125] training loss: 2.030197 validation loss: 2.208726\n","[6,  3125] training loss: 1.893452 validation loss: 1.960695\n","[7,  3125] training loss: 2.015880 validation loss: 1.746025\n","[8,  3125] training loss: 2.038822 validation loss: 2.275704\n","[9,  3125] training loss: 2.018946 validation loss: 2.594853\n","[10,  3125] training loss: 1.937749 validation loss: 1.711144\n","Finished Training\n"]}],"source":["for epoch in range(num_epochs):  # loop over the dataset multiple times\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        del inputs, labels, outputs\n","        torch.cuda.empty_cache()\n","        # print statistics\n","        running_loss += loss.item()\n","        \n","    with torch.no_grad():\n","        total_valid_loss = 0.0\n","        for images, labels in testloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            valid_outputs = model(images)\n","            valid_batch_loss = criterion(valid_outputs, labels)\n","            total_valid_loss += valid_batch_loss.item()\n","            \n","        test_batch_len = len(testloader)\n","        print(f'[{epoch + 1}, {i + 1:5d}] training loss: {running_loss / total_step:.6f} validation loss: {total_valid_loss / test_batch_len:.6f}')\n","        running_loss = 0.0\n","\n","print('Finished Training')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9 (default, Apr 13 2022, 08:48:06) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}}},"nbformat":4,"nbformat_minor":4}
